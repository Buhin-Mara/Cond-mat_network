{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "arxiv_get.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B6fSGO3F_ai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#参考”arxivの情報を使って特定分野の共著者ネットワークを書く”, Qiita \n",
        "#https://qiita.com/ek_ss/items/726319e2427202357fc4\n",
        "#Cond-matから対象年度の論文情報をcsv形式で取得する\n",
        "#データを取得済みの場合は次のセルから開始する\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math \n",
        "\n",
        "def month_string_to_number(string):\n",
        "    m = {\n",
        "        'jan': 1,\n",
        "        'feb': 2,\n",
        "        'mar': 3,\n",
        "        'apr':4,\n",
        "         'may':5,\n",
        "         'jun':6,\n",
        "         'jul':7,\n",
        "         'aug':8,\n",
        "         'sep':9,\n",
        "         'oct':10,\n",
        "         'nov':11,\n",
        "         'dec':12\n",
        "        }\n",
        "    s = string.strip()[:3].lower()\n",
        "\n",
        "    try:\n",
        "        out = m[s]\n",
        "        return out\n",
        "    except:\n",
        "        raise ValueError('Not a month')\n",
        "\n",
        "# Search result のページから検索結果件数を取得\n",
        "def get_number_of_searchresult(url):\n",
        "    html_doc = requests.get(url).text\n",
        "    soup = BeautifulSoup(html_doc, \"html.parser\") \n",
        "    tags = soup.find_all(\"h1\",{\"class\":\"title is-clearfix\"})\n",
        "    text = tags[0].text.strip()\n",
        "    if \"Showing\" in text and \"results\" in text:\n",
        "        stext = text.split(\" \")\n",
        "        datanum = int(stext[3].replace(',', ''))#検索結果の件数を取得\n",
        "    else:\n",
        "        datanum=math.nan\n",
        "    return datanum\n",
        "\n",
        "# Search resultsから情報取得しnp.ndarray化\n",
        "def collect_info_from_advancedsearch(urlhead, datanum,key):\n",
        "\n",
        "    titles=[]# データ保存用リスト\n",
        "    absts=[]#Abst\n",
        "    cites=[]# cite情報 (arxiv:XXXXXX)\n",
        "    authors=[]# author\n",
        "    dates=[]# 日付情報\n",
        "    dates_orig_m =[]#初回投稿月\n",
        "    dates_orig_y =[]#初回投稿年\n",
        "    dois = []#doi\n",
        "    fields=[]#cross-listを含む分野情報\n",
        "\n",
        "    startnum=0\n",
        "    while datanum > startnum:\n",
        "        print(str(startnum)+\"...\", end=\"\")\n",
        "        url = urlhead+str(startnum)#advanced searchのURL \n",
        "        html_doc = requests.get(url).text\n",
        "        soup = BeautifulSoup(html_doc, \"html.parser\") \n",
        "\n",
        "        # title情報\n",
        "        tags1= soup.find_all(\"p\", {\"class\": \"title is-5 mathjax\"}) \n",
        "        titles_tmp = [tag.text.strip() for tag in tags1]\n",
        "\n",
        "        # abst情報\n",
        "        tags2 = soup.find_all(\"span\", {\"class\": \"abstract-full has-text-grey-dark mathjax\"})\n",
        "        absts_tmp = [tag.text[:-7].strip() for tag in tags2 if \"Less\" in tag.text]\n",
        "\n",
        "        # cite情報\n",
        "        tags3 =soup.find_all(\"p\", {\"class\": \"list-title is-inline-block\"})\n",
        "        cites_tmp = [tag.select(\"a\")[0].text for tag in tags3]\n",
        "\n",
        "        # Date情報\n",
        "        tags4 = soup.find_all(\"p\",{\"class\":\"is-size-7\"})\n",
        "        text = [tag.text.strip() for tag in tags4 if \"originally announced\" in tag.text ]\n",
        "        dates_tmp = text\n",
        "        dates_orig_y_tmp=[txt.split(\" \")[-1][:-1] for txt in text]\n",
        "        dates_orig_m_tmp=[month_string_to_number(txt.split(\" \")[-2]) for txt in text]\n",
        "\n",
        "        # DOI情報\n",
        "        tags5 = soup.find_all(\"div\",{\"class\":\"is-marginless\"})\n",
        "        dois_tmp = [tag.text[tag.text.rfind(\"doi\"):].split(\"\\n\")[1] for tag in tags5 if key in tag.text ]   \n",
        "\n",
        "        # Author情報\n",
        "        tags6= soup.find_all(\"p\", {\"class\": \"authors\"}) \n",
        "        auths_tmp = []\n",
        "        for tag in tags6:\n",
        "            auths=tag.select(\"a\")\n",
        "            authlist=[(author.text,author.get(\"href\")[33:]) for author in auths]\n",
        "            auths_tmp.append(authlist)\n",
        "\n",
        "        # Cross-list情報\n",
        "        tags7= soup.find_all(\"div\", {\"class\": \"tags is-inline-block\"})  # title#(\"span\", {\"class\": \"tag is-small is-link tooltip is-tooltip-top\"})  # title\n",
        "        fields_tmp=[tag.text.strip().split(\"\\n\") for tag in tags7]\n",
        "\n",
        "        # 結果に追加\n",
        "        titles.extend(titles_tmp)\n",
        "        absts.extend(absts_tmp)\n",
        "        cites.extend(cites_tmp)\n",
        "        authors.extend(auths_tmp)\n",
        "        dates.extend(dates_tmp)\n",
        "        dates_orig_y.extend(dates_orig_y_tmp)\n",
        "        dates_orig_m.extend(dates_orig_m_tmp)\n",
        "        dois.extend(dois_tmp)\n",
        "        fields.extend(fields_tmp)\n",
        "\n",
        "        # 検索結果の次ページの開始番号更新\n",
        "        startnum += sizenum\n",
        "\n",
        "    nt = np.array(titles)\n",
        "    na = np.array(absts)\n",
        "    nauth = np.array(authors)\n",
        "    ncite = np.array(cites)\n",
        "    nd = np.array(dates)\n",
        "    ndy = np.array(dates_orig_y)\n",
        "    ndm = np.array(dates_orig_m)\n",
        "    ndoi = np.array(dois)\n",
        "    nfields=np.array(fields)\n",
        "    npdataset = np.concatenate([[ncite],[nt],[na],[nauth],[nfields],[ndoi],[ndy],[ndm],[nd]],axis=0).T\n",
        "    print(\" collected data number : \", npdataset.shape[0])\n",
        "    return npdataset\n",
        "\n",
        "#検索クエリの検索対象分類の指定用辞書\n",
        "dict_class={'cs': '&classification-computer_science=y',\n",
        " 'econ': '&classification-economics=y',\n",
        " 'eess': '&classification-eess=y',\n",
        " 'math': '&classification-mathematics=y',\n",
        " 'q-bio': '&classification-q_biology=y',\n",
        " 'q-fin': '&classification-q_finance=y',\n",
        " 'stat': '&classification-statistics=y',\n",
        " 'all': '&classification-physics=y&classification-physics_archives=all',\n",
        " 'astro-ph': '&classification-physics=y&classification-physics_archives=astro-ph',\n",
        " 'cond-mat': '&classification-physics=y&classification-physics_archives=cond-mat',\n",
        " 'gr-qc': '&classification-physics=y&classification-physics_archives=gr-qc',\n",
        " 'hep-ex': '&classification-physics=y&classification-physics_archives=hep-ex',\n",
        " 'hep-lat': '&classification-physics=y&classification-physics_archives=hep-lat',\n",
        " 'hep-ph': '&classification-physics=y&classification-physics_archives=hep-ph',\n",
        " 'hep-th': '&classification-physics=y&classification-physics_archives=hep-th',\n",
        " 'math-ph': '&classification-physics=y&classification-physics_archives=math-ph',\n",
        " 'nlin': '&classification-physics=y&classification-physics_archives=nlin',\n",
        " 'nucl-ex': '&classification-physics=y&classification-physics_archives=nucl-ex',\n",
        " 'nucl-th': '&classification-physics=y&classification-physics_archives=nucl-th',\n",
        " 'physics': '&classification-physics=y&classification-physics_archives=physics',\n",
        " 'quant-ph': '&classification-physics=y&classification-physics_archives=quant-ph'}\n",
        "\n",
        "years = [y for y in range(2015,2016)]\n",
        "key = 'cond-mat'#検索対象の分野　dict_classのkeyを指定\n",
        "output_fname=\"df_cond-mat\" #出力ファイル名\n",
        "\n",
        "url0=\"https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term=&terms-0-field=title\"\n",
        "url1=\"&classification-include_cross_list=include\"\n",
        "url_daterange=\"&date-year=&date-filter_by=date_range\"\n",
        "url2=\"&date-date_type=submitted_date&abstracts=show&size=\"\n",
        "urlmid = \"&order=-announced_date_first&start=\"\n",
        "\n",
        "sizenum = 25\n",
        "startnum=0\n",
        "for year in years:\n",
        "    m_divide = 1 # 検索期間の分割数\n",
        "    mstart =1\n",
        "    mstop = 1\n",
        "    url_date = \"&date-from_date=\"+str(year)+\"-\"+str(mstart).zfill(2)+\"-01&date-to_date=\"+str(year+1)+\"-\"+str(mstop).zfill(2)+\"-01\"\n",
        "    urlhead = url0+dict_class[key]+url1+url_daterange+url_date+url2\n",
        "    urlmid = \"&order=-announced_date_first&start=\"\n",
        "    url = urlhead+str(sizenum)+urlmid+str(startnum)\n",
        "\n",
        "    datanum=get_number_of_searchresult(url) #検索結果件数の取得\n",
        "    print(\"Number of search results (\"+str(year)+\") : \"+str(datanum))\n",
        "\n",
        "    if datanum >=10000: #件数がLimitを超えた場合、1年間のデータ取得を月ごとに分割\n",
        "        m_divide = 13\n",
        "        for month_divide in range(2,12): #個別件数がLimit以下になる分割数の探索\n",
        "            flag_numlimit = False\n",
        "            for idx in range(month_divide):\n",
        "                mstart = int(idx*12/month_divide+1)\n",
        "                mstop = (int((idx+1)*12/month_divide)+1)%12\n",
        "                if mstop !=1:\n",
        "                    url_date = \"&date-from_date=\"+str(year)+\"-\"+str(mstart).zfill(2)+\"-01&date-to_date=\"+str(year)+\"-\"+str(mstop).zfill(2)+\"-01\"\n",
        "                else:\n",
        "                    url_date = \"&date-from_date=\"+str(year)+\"-\"+str(mstart).zfill(2)+\"-01&date-to_date=\"+str(year+1)+\"-\"+str(mstop).zfill(2)+\"-01\"\n",
        "\n",
        "                urlhead = url0+dict_class[key]+url1+url_daterange+url_date+url2\n",
        "                url = urlhead+str(sizenum)+urlmid+str(startnum)\n",
        "                datanum=get_number_of_searchresult(url)#分割数ごとでのデータ件数取得\n",
        "                if datanum >= 10000:\n",
        "                    flag_numlimit = True\n",
        "            if not flag_numlimit:\n",
        "                m_divide = month_divide\n",
        "                break\n",
        "        if m_divide > 12:\n",
        "            print(\"*** Number of search result is over the limit 10,000. Please refine your search. ***\")\n",
        "\n",
        "    sizenum=200\n",
        "    npdataset = np.empty((0,9))\n",
        "    for idx in range(m_divide):\n",
        "        mstart = int(idx*12/m_divide+1)\n",
        "        mstop = (int((idx+1)*12/m_divide)+1)%12\n",
        "        if mstop !=1:\n",
        "            url_date = \"&date-from_date=\"+str(year)+\"-\"+str(mstart).zfill(2)+\"-01&date-to_date=\"+str(year)+\"-\"+str(mstop).zfill(2)+\"-01\"\n",
        "        else:\n",
        "            url_date = \"&date-from_date=\"+str(year)+\"-\"+str(mstart).zfill(2)+\"-01&date-to_date=\"+str(year+1)+\"-\"+str(mstop).zfill(2)+\"-01\"\n",
        "\n",
        "        urlhead = url0+dict_class[key]+url1+url_daterange+url_date+url2       \n",
        "        url = urlhead+str(25)+urlmid+str(0)\n",
        "        datanum=get_number_of_searchresult(url)\n",
        "\n",
        "        print(\"Collect search results ...\" + url_date + \", Number of search results : \" + str(datanum))\n",
        "        urlhead2 = urlhead+str(sizenum)+urlmid\n",
        "        npdataset_tmp = collect_info_from_advancedsearch(urlhead2,datanum,key)\n",
        "        npdataset = np.concatenate([npdataset, npdataset_tmp], axis=0)\n",
        "\n",
        "    #1年分の情報をnumpy、pandas DataFrameに変換してcsvに保存\n",
        "    dataset = pd.DataFrame(npdataset)\n",
        "    dataset.columns =[\"Cite\",\"Title\",\"Abst\",\"Authors\",\"Fields\",\"DOI\", \"OrigDateY\",\"OrigDateM\",\"Date Info\"]\n",
        "    dataset.to_csv(output_fname+str(year)+\".csv\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "was_u_3a6Gd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://qiita.com/ek_ss/items/726319e2427202357fc4\n",
        "#必要な年度のデータがcsv形式で保存されている場合はこのセルから開始する。\n",
        "#必要なライブラリのimport\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math \n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import networkx as nx\n",
        "import collections\n",
        "\n",
        "import itertools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR83De8zTZbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#テキストデータの処理\n",
        "def preprocess_text(text):\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)#Remove punctuations\n",
        "    text = text.lower()\n",
        "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)#remove tags\n",
        "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)# remove special characters and digits\n",
        "\n",
        "    text = text.split()\n",
        "\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    ##Stemming\n",
        "    #Lemmatisation\n",
        "    ps=PorterStemmer()\n",
        "    #lem = WordNetLemmatizer()\n",
        "    text = [ps.stem(word) for word in text if not word in stop_words]\n",
        "    text=\" \".join(text)\n",
        "\n",
        "    return text\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FOxWmBkQkSc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#処理する複数年度のcsvデータを結合して一つのデータとする。\n",
        "fname_head = \"df_cond-mat\"\n",
        "\n",
        "fname =\"/content/\"+ fname_head + str(2019)+\".csv\"\n",
        "dataset = pd.read_csv(fname, index_col=0)\n",
        "\n",
        "for year in range(2015,2019):\n",
        "    fname = \"/content/\" + fname_head + str(year)+\".csv\"\n",
        "    print(fname)\n",
        "    dataset_tmp = pd.read_csv(fname, index_col=0)\n",
        "    dataset = pd.concat([dataset,dataset_tmp])\n",
        "\n",
        "ndata = (dataset['Title']+\" \"+ dataset['Abst']).values\n",
        "ndata= np.array([preprocess_text(text) for text in ndata])\n",
        "dataset['Keywords'] = ndata\n",
        "dataset =dataset.reset_index()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtVKhF46LZVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#検索するキーワードの設定。アブストラクトとタイトルにキーワードが含まれる論文を抽出する。複数キーワードにも拡張可能なはず。\n",
        "word=\"topolo\"\n",
        "dataset_r=dataset[dataset['Keywords'].str.contains(word)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLLtCl0XZgWE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(dataset_r.head()) #冒頭５つの情報\n",
        "print(dataset_r.shape) #抽出した単語の論文数\n",
        "print(dataset.shape) #全論文数"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ai0IBlzjYh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#抽出した論文に含まれる各著者とその論文数を抽出する\n",
        "%%time\n",
        "\n",
        "auths= dataset_r['Authors'].values\n",
        "datanum = auths.shape[0]\n",
        "\n",
        "auth_dict = {} \n",
        "pattern = re.compile(r\"(\\()(.*?)\\)\")\n",
        "for idx in range(datanum):\n",
        "    match = re.findall(pattern,auths[idx])\n",
        "    for m in match:\n",
        "        auth = m[1].split(m[1][-1])[-2]\n",
        "        authname = m[1].split(m[1][0])[1]\n",
        "        auth_dict.setdefault(auth,[])\n",
        "        auth_dict[auth].append(authname)\n",
        "\n",
        "#論文数有りの場合\n",
        "for key,value in auth_dict.items():\n",
        "    count = collections.Counter(value)\n",
        "    auth_dict[key]=(count.most_common()[0][0],len(value))\n",
        "\n",
        "print(auth_dict)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9fLUpMd37zK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#グラフに情報追加\n",
        "G=nx.DiGraph()\n",
        "for idx in range(datanum):\n",
        "    match = re.findall(pattern,auths[idx])\n",
        "    for m in match:\n",
        "        auth = m[1].split(m[1][-1])[-2]\n",
        "        #authname = m[1].split(m[1][0])[1]#表記のままでグラフ作成したい場合はこのauthnameで辺を追加する\n",
        "        G.add_edges_from([(idx,auth_dict[auth][0])])\n",
        "        #G.add_edges_from([(idx,auth_dict[authname])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIojcc5RJdqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#著者のArxiv表記とフルネーム、論文数をcsvとして保存する\n",
        "dict_pd=pd.DataFrame(auth_dict)\n",
        "dict_pd=dict_pd.T\n",
        "dict_pd.columns=[\"authername\", \"paper\"]\n",
        "dict_pd[\"paper\"]=dict_pd[\"paper\"].astype(int)\n",
        "dict_pd=dict_pd.sort_values(by=\"paper\",ascending=False)\n",
        "print(dict_pd)\n",
        "dict_pd.to_csv(\"dictionary_\"+word+\".csv\", encoding=\"utf_8\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01fQbDNsW6Rd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#共同研究者ネットワークの作成。全員ふくめるとネットワークが混雑するので、エッジ数(共同研究者数)の閾値を設定する\n",
        "thr=80 #繋がり数thr未満の人は削除、\n",
        "authorlist = [n  for n in G.nodes() if type(n) is str]\n",
        "G_removed=G.copy()\n",
        "for auth in authorlist:\n",
        "    deg=G_removed.degree(auth)\n",
        "    if deg <=thr:\n",
        "        G_removed.remove_node(auth)\n",
        "\n",
        "#上記の結果、次数がゼロになった著者を削除。その人とつながっていた人分人数減\n",
        "for idx in range(datanum):\n",
        "    deg=G_removed.degree(idx)\n",
        "    if deg <=0:\n",
        "        G_removed.remove_node(idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuuIva05W98q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ネットワークの作成\n",
        "def draw_graph(G, label = False):\n",
        "    # pagerank の計算\n",
        "    pr = nx.pagerank(G)\n",
        "    pos = nx.spring_layout(G)\n",
        "\n",
        "    fig = plt.figure(figsize=(8.0, 6.0))\n",
        "    c=[(0.4,0.4,1) if type(n) is str else (1,0.4,0.4)  for n in G.nodes()]\n",
        "\n",
        "    nx.draw_networkx_edges(G,pos, edge_color=(0.3,0.3,0.3))\n",
        "    nx.draw_networkx_nodes(G,pos, node_color=c, node_size=[5000*v for v in pr.values()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMHR8ZrUeNzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ネットワークのエッジに共同研究数の重みをつける\n",
        "def convert_weightedGraph(graph):\n",
        "    graph_new =nx.Graph()\n",
        "    for node in graph.nodes:\n",
        "        if type(node) is str:\n",
        "            continue\n",
        "        n_new = [e[1] for e in graph.edges if e[0]==node]\n",
        "        for e_new in itertools.permutations(n_new, 2):\n",
        "\n",
        "            flag_dup = False\n",
        "            for e_check in graph_new.edges(data=True):\n",
        "                if e_new[0] in e_check and e_new[1] in e_check:\n",
        "                    e_check[2]['weight']+=1\n",
        "                    flag_dup = True\n",
        "            if not flag_dup:\n",
        "                graph_new.add_edge(e_new[0],e_new[1],weight=1)\n",
        "    return graph_new\n",
        "\n",
        "wG=convert_weightedGraph(G_removed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjvgC04WBjUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#完成したネットワーク上の研究者名とエッジ数をcsvで保存する。\n",
        "record=pd.DataFrame(wG.degree, columns=['Author', 'Edge'])\n",
        "record=record.sort_values(by='Edge',ascending=False)\n",
        "print(record.head())\n",
        "record.to_csv(\"wEdge_\"+word+\".csv\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVITbKqTj5vy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ネットーワークを図にする\n",
        "def draw_weightedG(G):\n",
        "    fig = plt.figure(figsize=(8.0, 6.0))\n",
        "    pr = nx.pagerank(G)\n",
        "    k0=20 #ノード間距離の調節数値\n",
        "    pos = nx.fruchterman_reingold_layout(G,k=k0/math.sqrt(G.order()))\n",
        "    #pos = nx.spring_layout(G,k=15/math.sqrt(G.order())) \n",
        "    #適当なlayoutを使う。kの値でノード間距離を微調整  \n",
        "\n",
        "\n",
        "    x_values, y_values = zip(*pos.values())\n",
        "    x_max = max(x_values)\n",
        "    x_min = min(x_values)\n",
        "    x_margin = (x_max - x_min) * 0.25\n",
        "    plt.xlim(x_min - x_margin, x_max + x_margin) #ラベル文字が切れないようにマージン確保\n",
        "\n",
        "    node_sizes=[5000*v for v in pr.values()]\n",
        "    edge_colors = [e[2]['weight'] for e in G.edges(data=True)] #辺の重みで色付け\n",
        "    nodes = nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='#9999ff')\n",
        "    edges = nx.draw_networkx_edges(G, pos, node_size=node_sizes, arrowstyle='->',\n",
        "                                   arrowsize=10, edge_color=edge_colors,\n",
        "                                   edge_cmap=plt.cm.Blues, width=2)\n",
        "    nx.draw_networkx_labels(G,pos)\n",
        "\n",
        "    ax = plt.gca()\n",
        "    ax.set_axis_off()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1bGhq8AuC12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ネットワーク上の研究者名とエッジ数のリスト\n",
        "print(list(G.edges))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SvnLanxeamw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#図にする\n",
        "draw_weightedG(wG)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mo9yfs3cgyHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#連結かどうかで部分グラフに分割する\n",
        "def add_edges_to_wsubgraph(subg, edge_new,node,edges_all):\n",
        "    subg.add_edges_from([edge_new])\n",
        "\n",
        "    if node == edge_new[1]:\n",
        "        node_new = edge_new[0]\n",
        "    else:\n",
        "        node_new = edge_new[1]\n",
        "\n",
        "    edges_new = [e for e in edges_all if node_new in e and e not in subg.edges(data=True)]\n",
        "    for edge in edges_new:\n",
        "        if edge not in subg.edges(data=True):\n",
        "            add_edges_to_wsubgraph(subg,edge,node_new,edges_all)\n",
        "\n",
        "def separate_wG_by_connectivity(G):\n",
        "    nodes_all =[n for n in G.nodes()]\n",
        "    edges_all = [e for e in G.edges(data=True)]\n",
        "    subgraphs = []\n",
        "\n",
        "    for node in nodes_all:\n",
        "        usedflag = any([node in g.nodes for g in subgraphs])\n",
        "        if usedflag:\n",
        "            continue \n",
        "\n",
        "        subg = nx.Graph()\n",
        "        subg.add_node(node)\n",
        "        edges_new = [e for e in edges_all if node in e]\n",
        "        for edge in edges_new:\n",
        "            if edge not in subg.edges(data=True):\n",
        "                add_edges_to_wsubgraph(subg,edge,node,edges_all)\n",
        "        subgraphs.append(subg)\n",
        "\n",
        "    return subgraphs\n",
        "\n",
        "subgraphs = separate_wG_by_connectivity(wG)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmSmfXdkg3sQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#図にする\n",
        "\n",
        "draw_weightedG(subgraphs[0])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBIbQfuQA-BM",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlKnKcoIApo7",
        "colab_type": "text"
      },
      "source": [
        "# 新しいセクション"
      ]
    }
  ]
}